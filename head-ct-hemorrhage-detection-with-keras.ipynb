{"cells":[{"metadata":{"trusted":false,"_uuid":"8b9a74f00b6d760a57037c5c885e338c072a741b"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport glob\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51272c641ae50dbbd0437d4a7b9761cffff0d4c6"},"cell_type":"code","source":"labels_df = pd.read_csv('../input/labels.csv')\nlabels = np.array(labels_df[' hemorrhage'].tolist())\n\nfiles = sorted(glob.glob('../input/head_ct/head_ct/*.png'))\nimages = np.array([cv2.imread(path) for path in files])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62ca590698a95bf23788e8797588b4aac1313162"},"cell_type":"markdown","source":"# Initial data exploration"},{"metadata":{"trusted":false,"_uuid":"c4be12f3b28ebeed5eb893e4e61f2e99a0134e13"},"cell_type":"code","source":"labels_df[' hemorrhage'].hist(bins=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b81413310e579183991f7d2a69854aa7639ccd3"},"cell_type":"markdown","source":"There is the same amount of data for both cases."},{"metadata":{"_uuid":"2e1fbdddb5c16241fb668a83c6603f50009ee9f5"},"cell_type":"markdown","source":"Images are not the same sizes! We need to find the optimal size, but before we have to explore it."},{"metadata":{"trusted":false,"_uuid":"e491053dd6c4055164fc16802066b3997f84f4a2"},"cell_type":"code","source":"images_df = pd.DataFrame(images, columns=['image'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"85811a2bcce98eb06d6ff128a3320b1e7df71632"},"cell_type":"code","source":"images_df['width'] = images_df['image'].apply(lambda x: x.shape[0])\nimages_df['height'] = images_df['image'].apply(lambda x: x.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba7ce285d08e9caf3b9931b07dd61f021b04b797"},"cell_type":"code","source":"images_df[['height', 'width']].hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6b4943ef0a1aa75255ef80e4fde33ba1142dcb0e"},"cell_type":"code","source":"images_df[['height', 'width']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ee66e3831882fa2eb5931ba1fe5d75fecfa0e4d"},"cell_type":"markdown","source":"Before we will create and train model, we need to make all images the same sizes.\n\nThe tradeoff is simple here - lesser images would be faster to train, there would be a lot of examples so lesser chance of overfitting, but it is a clear loss of information. If the error would be still big, we will need to consider to use a bigger size, and either stretch little images (and lose quality significantly) or drop them entirely (and risk overfitting).\n\nFor now we will go the simplest path - resizing to the smallest size (and even smaller - 128 insted of 134)."},{"metadata":{"trusted":false,"_uuid":"2a08824c6d4a48bff9eeef6b6027d83bd691c35c"},"cell_type":"code","source":"images = np.array([cv2.resize(image, (128, 128)) for image in images])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"28b14eb2838808af2e44c0cbfdf6cd0cc270a1e7"},"cell_type":"code","source":"plt.imshow(images[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"702f873c8325f344d04446e1648f5aa3ebdb330a"},"cell_type":"code","source":"plt.imshow(images[100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720c94b1d7a4390d46873777195e9a255612a37b"},"cell_type":"markdown","source":"The quality of images seems to be acceptable."},{"metadata":{"_uuid":"1ed7532db3f0ec920a9b0b43a54533eea5f576ab"},"cell_type":"markdown","source":"# Adding flipped images"},{"metadata":{"_uuid":"4bf20f0a4162c4e58677050373e9cf6236586b4a"},"cell_type":"markdown","source":"We could also improve the dataset by adding flipped images. It doesn't matter from what side we will look at the CT scan, brain hemorrhage can and should be diagnosed just as well. By adding flipped images to dataset, we can greatly increase the accuracy of model."},{"metadata":{"trusted":false,"_uuid":"af57d2df0d0753548b1ebcf933189c9b4fb36231"},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor i, flip in enumerate([None, -1, 0, 1]):\n    plt.subplot(221 + i)\n    if flip is None:\n        plt.imshow(images[0])\n    else:\n        plt.imshow(cv2.flip(images[0], flip))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9399b7e8949a5f26175fc6098cf09025d95c5b75"},"cell_type":"markdown","source":"Now, we don't want those flipped images in our test set just to be sure model didn't create any preferences for upside down and flipped images, so the dataset expansion should take place after split into train and test sets.\n\nFortunately, there is ImageDataGenerator for the purposes of flipping and rotating images."},{"metadata":{"_uuid":"7efecc340ab1995e75ea4cbd13b0893a45af7180"},"cell_type":"markdown","source":"Split data into train, validation and test subsets."},{"metadata":{"trusted":false,"_uuid":"105fb2c8f66b80d171d1f4ff628c37d285816ca1"},"cell_type":"code","source":"print(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c47196d0649531b204503285e41bf050ac5ab762"},"cell_type":"code","source":"# since data is strictly true until index 100 and then strictly false,\n# we can take random 90 entries from frist half and then random 90 from the second half\n# to have evenly distributed train and test sets\nindicies = np.random.permutation(100)\ntrain_true_idx, test_true_idx = indicies[:90], indicies[90:]\ntrain_false_idx, test_false_idx = indicies[:90] + 100, indicies[90:] + 100\ntrain_idx, test_idx = np.append(train_true_idx, train_false_idx), np.append(test_true_idx, test_false_idx)\n\ntrain_validationX, train_validationY = images[train_idx], labels[train_idx]\ntestX, testY = images[test_idx], labels[test_idx]\n\nprint(train_validationX.shape, testX.shape)\nprint(train_validationY.shape, testY.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"55238c0486350744fc6df30330bf510f893fac54"},"cell_type":"code","source":"# now to split train and validation sets\ntr_len = train_validationX.shape[0]\ntrain_val_split = int(tr_len*0.9)\nindicies = np.random.permutation(tr_len)\ntrain_idx, validation_idx = indicies[:train_val_split], indicies[train_val_split:]\n\ntrainX, trainY = train_validationX[train_idx], train_validationY[train_idx]\nvalidationX, validationY = train_validationX[validation_idx], train_validationY[validation_idx]\n\nprint(trainX.shape, validationX.shape)\nprint(trainY.shape, validationY.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"441a15a29a523a5b6325948fdbf9eb2eb7fe6153"},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81e076f835947073467cb5d89bc8cb377f90b456"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Input, Flatten, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\n\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8548a8996cc2093399a3b7a63ddd0f7e734c6a7e"},"cell_type":"markdown","source":"# Image augmentation"},{"metadata":{"trusted":false,"_uuid":"fb049069ee15d4771a54e891e9ff82d512fa6484"},"cell_type":"code","source":"train_image_data = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.,\n    zoom_range=0.05,\n    rotation_range=180,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='constant',\n    cval=0\n)\nvalidation_image_data = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.,\n    zoom_range=0.05,\n    rotation_range=90,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='constant',\n    cval=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"134fe798545cc4b23465b4138e5239db49e8a50e"},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor X_batch, y_batch in train_image_data.flow(trainX, trainY, batch_size=9):\n    for i in range(0, 9):\n        plt.subplot(330 + 1 + i)\n        plt.imshow(X_batch[i])\n    plt.show()\n    break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fadf7acd285d3ac9d4233831ed6ef2da4037f466"},"cell_type":"markdown","source":"# Building the model"},{"metadata":{"trusted":false,"_uuid":"1a2723eb634f5768321b7c4546292f0954bbbada"},"cell_type":"code","source":"def check_accuracy(model, setX, actual, print_images=True):\n    predicted = np.array([int(x[0] > 0.5) for x in model.predict(setX)])\n    if print_images:\n        rows = math.ceil(len(predicted)/10.)\n        plt.figure(figsize=(20, 3 * rows))\n        for i in range(len(predicted)):\n            plt.subplot(rows, 10, i+1)\n            plt.imshow(setX[i])\n            plt.title(\"pred \"+str(predicted[i])+\" actual \"+str(actual[i]))\n        \n    confusion = confusion_matrix(actual, predicted)\n    tn, fp, fn, tp = confusion.ravel()\n    print(\"True positive:\", tp, \", True negative:\", tn,\n          \", False positive:\", fp, \", False negative:\", fn)\n\n    print(\"Total accuracy:\", np.sum(predicted==actual) / len(predicted) * 100., \"%\")\n    return (tn, fp, fn, tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"45b351d7ece688d9960fa1d883639ccd0cdefdb0"},"cell_type":"code","source":"def simple_conv_model(input_shape):\n    model = Sequential()\n    \n    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'))\n    \n    model.add(GlobalAveragePooling2D())\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3860aa96b967acdeff08e152282a5f3362fcd7d8"},"cell_type":"code","source":"model = simple_conv_model((128, 128, 3))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89c8c807388c0577f029a3d609fd2e4895cb26f3"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bc662aed6cc21c81c5f1af0cb8ce554bb4f90a3"},"cell_type":"markdown","source":"# Training model"},{"metadata":{"trusted":false,"_uuid":"6b559e9e337732a83d875102e5c0c132f904c60c"},"cell_type":"code","source":"model.fit_generator(train_image_data.flow(trainX, trainY, batch_size=128),\n    steps_per_epoch=128,\n    validation_data=validation_image_data.flow(validationX, validationY, batch_size=16),\n    validation_steps=100,\n    callbacks=[ModelCheckpoint(\"weights.h5\", monitor='val_acc', save_best_only=True, mode='max')],\n    epochs=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a258ab7dbeb8581ddf8df4f9cf6fb35565af98d7"},"cell_type":"code","source":"check_accuracy(model, validationX/255., validationY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2a3de977af6ebdc657051713f36611734acdc5e3"},"cell_type":"code","source":"model.save(\"last-weights.h5\")\nmodel.load_weights(\"weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a899ff7e601793307fa166de2fd255c716e1a4c9"},"cell_type":"code","source":"check_accuracy(model, trainX/255., trainY, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"862b8a81bcea3a4556b4fbcc0b3eff59949486b6"},"cell_type":"code","source":"check_accuracy(model, validationX/255., validationY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54fad30ac253ba4f2af7d716324a21c78542efc8"},"cell_type":"markdown","source":"The overall generalization of model seems good, overfitting isn't too big. But since this is a medical problem, we have to consider one additional thing."},{"metadata":{"_uuid":"e9185db7a5d065d37821fe9ea3b7f1f46b812669"},"cell_type":"markdown","source":"# False negative result will kill patient\nFalse positive result will be an inconvinience.\n\nWe have to punish false negative results while training the model."},{"metadata":{"_uuid":"fcc9e01ebfb72285230f57b04b7f487997fc89bd"},"cell_type":"markdown","source":"Punishing false negatives may be implemented in several ways.\n* imbalance dataset so there are more positive cases, therefore model will prefer false positives over false negatives\n* make it a multiclass classification and use 'class_weight' parameter of Keras (which is essentially will do the same trick)\n* write custom loss function that is oriented on lowering false negativer rate (or improving _sensitivity_)\n* or write custom metrics, based on which checkpoint will save model\n\nLet's try the approach with imbalancing training dataset."},{"metadata":{"trusted":false,"_uuid":"d44bc961ecb46e77851c956d31a7aa161d6965fc"},"cell_type":"code","source":"def imbalance_set(coeff=2):\n    imbalanced_trainX = []\n    imbalanced_trainY = []\n    for i, train_x in enumerate(trainX):\n        def add_entry(x, y):\n            imbalanced_trainX.append(x)\n            imbalanced_trainY.append(y)\n\n        add_entry(train_x, trainY[i])\n\n        if(trainY[i] == 1):\n            for j in range(coeff-1):\n                add_entry(train_x, trainY[i])\n    return (np.array(imbalanced_trainX), np.array(imbalanced_trainY))\n\nimbalanced_trainX, imbalanced_trainY = imbalance_set(2)\nprint(imbalanced_trainX.shape, imbalanced_trainY.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3179669f085dd87243ea141d95e84155139cdb91"},"cell_type":"code","source":"def bigger_conv_model(input_shape):\n    model = Sequential()\n    \n    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu', input_shape=input_shape))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'))\n    \n    model.add(GlobalAveragePooling2D())\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c6e37adf00370297187302ad4dc085b0bbcfc638"},"cell_type":"code","source":"model = bigger_conv_model((128, 128, 3))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13cba9606ec6302550f84de8c562dd4d26336a87"},"cell_type":"raw","source":"model.fit(imbalanced_trainX, imbalanced_trainY, validation_data=(validationX, validationY),\n          callbacks=[ModelCheckpoint(\"weights-fna-model.hdf5\", monitor='val_acc', save_best_only=True, mode='max')],\n          batch_size=128, epochs=200)"},{"metadata":{"trusted":false,"_uuid":"12aca8ca335e6a28e3f7c4ce10866562c4a826bb"},"cell_type":"code","source":"model.fit_generator(train_image_data.flow(imbalanced_trainX, imbalanced_trainY, batch_size=128),\n    steps_per_epoch=128,\n    validation_data=validation_image_data.flow(validationX, validationY, batch_size=16),\n    validation_steps=100,\n    callbacks=[ModelCheckpoint(\"bigger_model_checkpoint_weights.h5\", monitor='val_acc', save_best_only=True, mode='max')],\n    epochs=24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"545b13ba4d826889b7d77ffccda0a3201017c677"},"cell_type":"code","source":"check_accuracy(model, trainX/255., trainY, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"885a01e233fdb3b3f507d1b04a0e470227eeab92"},"cell_type":"code","source":"check_accuracy(model, validationX/255., validationY, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8996c517217202e55fb3d6d1cef368074890800f"},"cell_type":"code","source":"model.save(\"bigger_model_latest_weights.h5\")\nmodel.load_weights(\"bigger_model_checkpoint_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"76773eb68bcb2d9a6e61ff4e7aa4490b0e4adebb"},"cell_type":"code","source":"check_accuracy(model, trainX/255., trainY, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd79d3c72cd34527aec635c6654c108dc26583f7"},"cell_type":"code","source":"check_accuracy(model, validationX/255., validationY, False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae7f2e21ee8143ece199abf0add11a2e471207be"},"cell_type":"markdown","source":"# 89% of accuracy on validation set and 0 false negative\nTime to check model on test set"},{"metadata":{"trusted":false,"_uuid":"045b9628753496fc5801e15ae2966cc58704b525"},"cell_type":"code","source":"check_accuracy(model, testX/255., testY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67606f2101175d452e1b54cdd5dd08f8faca6ce3"},"cell_type":"markdown","source":"Model showed good results.\n\nAdditional improvements could be made if image augmentation contained alterations of contrast."},{"metadata":{"trusted":false,"_uuid":"a4352711ac7bb41c22739fa88ca9037fc90455c1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}